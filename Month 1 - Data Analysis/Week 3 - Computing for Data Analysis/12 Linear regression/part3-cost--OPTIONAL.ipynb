{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "# Part 3: The cost of solving the normal equations\n",
                "\n",
                "This notebook helps you explore the execution time cost of solving the normal equations,\n",
                "\n",
                "$$\n",
                "  X^T X \\theta^* = X^T y.\n",
                "$$\n",
                "\n",
                "This notebook only has one exercise, but it is not graded. So, you should complete it for your own edification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## Scalability with the problem size\n",
                "\n",
                "To start, here is some code to help generate synthetic problems of a certain size, namely, $m \\times (n+1)$, where $m$ is the number of observations and $n$ the number of predictors. The $+1$ comes from our usual dummy coefficient for a non-zero intercept.\n",
                "\n",
                "We will also implement a linear least squares solver, `estimate_coeffs()`, that simply calls Numpy's `lstsq()` routine."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def generate_model (n):\n",
                "    \"\"\"Returns a set of (random) n+1 linear model coefficients.\"\"\"\n",
                "    return np.random.rand (n+1, 1)\n",
                "\n",
                "def generate_data (m, theta, sigma=1.0\/(2**0.5)):\n",
                "    \"\"\"\n",
                "    Generates 'm' noisy observations for a linear model whose\n",
                "    predictor (non-intercept) coefficients are given in 'theta'.\n",
                "    Decrease 'sigma' to decrease the amount of noise.\n",
                "    \"\"\"\n",
                "    assert (type (theta) is np.ndarray) and (theta.ndim == 2) and (theta.shape[1] == 1)\n",
                "    n = len (theta)\n",
                "    X = np.random.rand (m, n)\n",
                "    X[:, 0] = 1.0\n",
                "    y = X.dot (theta) + sigma*np.random.randn (m, 1)\n",
                "    return (X, y)\n",
                "\n",
                "def estimate_coeffs(X, y):\n",
                "    \"\"\"\n",
                "    Solves X*theta = y by a linear least squares method.\n",
                "    \"\"\"\n",
                "    result = np.linalg.lstsq (X, y, rcond = None)\n",
                "    theta = result[0]\n",
                "    return theta"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Demo the above routines for a 2-D dataset.\n",
                "\n",
                "m = 50\n",
                "theta_true = generate_model (1)\n",
                "(X, y) = generate_data (m, theta_true, sigma=0.1)\n",
                "\n",
                "print (\"Dimensions of X:\", X.shape)\n",
                "print (\"Dimensions of theta_true:\", theta_true.shape)\n",
                "print (\"Dimensions of y:\", y.shape)\n",
                "\n",
                "print (\"Condition number of X: \", np.linalg.cond (X))\n",
                "print (\"True model coefficients:\", theta_true.T)\n",
                "\n",
                "theta = estimate_coeffs (X, y)\n",
                "\n",
                "print (\"Estimated model coefficients:\", theta.T)\n",
                "\n",
                "fig = plt.figure()\n",
                "ax1 = fig.add_subplot(111)\n",
                "ax1.plot (X[:, 1], y, 'b+') # Noisy observations\n",
                "ax1.plot (X[:, 1], X.dot (theta), 'r*') # Fit\n",
                "ax1.plot (X[:, 1], X.dot (theta_true), 'go') # True solution"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Benchmark varying $m$.** Let's benchmark the time to compute $x$ when the dimension $n$ of each point is fixed but the number $m$ of points varies. How does the running time scale with $m$?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# Benchmark, as 'm' varies:\n",
                "\n",
                "n = 32 # dimension\n",
                "M = [100, 1000, 10000, 100000, 1000000]\n",
                "times = [0.] * len (M)\n",
                "for (i, m) in enumerate (M):\n",
                "    theta_true = generate_model (n)\n",
                "    (X, y) = generate_data (m, theta_true, sigma=0.1)\n",
                "    t = %timeit -o estimate_coeffs (X, y)\n",
                "    times[i] = t.best"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "t_linear = [times[0]\/M[0]*m for m in M]\n",
                "\n",
                "fig = plt.figure()\n",
                "ax1 = fig.add_subplot(111)\n",
                "ax1.loglog (M, times, 'bo')\n",
                "ax1.loglog (M, t_linear, 'r--')\n",
                "ax1.set_xlabel ('m (number of observations)')\n",
                "fig.suptitle ('Running time (fixed number of predictors)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Exercise 0** (ungraded). Now fix the number $m$ of observations but vary the dimension $n$. How does time scale with $n$? Complete the benchmark code below to find out. In particular, given the array `N[:]`, compute an array, `times[:]`, such that `times[i]` is the running time for a problem of size `m`$\\times$`(N[i]+1)`.\n",
                "\n",
                "> Hint: You can adapt the preceding benchmark. Also, note that the code cell following the one immediately below will plot your results against $\\mathcal{O}(n)$ and $\\mathcal{O}(n^2)$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "bench_n",
                    "locked": false,
                    "schema_version": 1,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "N = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
                "m = 5000\n",
                "times = [0.] * len (N)\n",
                "\n",
                "# Implement a benchmark to compute the time,\n",
                "# `times[i]`, to execute a problem of size `N[i]`.\n",
                "for (i, n) in enumerate (N):\n",
                "    ###\n",
                "    ### YOUR CODE HERE\n",
                "    ###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "cell-1c7b19e5b62b895c",
                    "locked": true,
                    "points": 0,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "t_linear = [times[0]\/N[0]*n for n in N]\n",
                "t_quadratic = [times[0]\/N[0]\/N[0]*n*n for n in N]\n",
                "\n",
                "fig = plt.figure()\n",
                "ax1 = fig.add_subplot(111)\n",
                "ax1.loglog (N, times, 'bo')\n",
                "ax1.loglog (N, t_linear, 'r--')\n",
                "ax1.loglog (N, t_quadratic, 'g--')\n",
                "ax1.set_xlabel ('n (number of predictors)')\n",
                "fig.suptitle ('Running time (fixed number of observations)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "Thus, the empirical scaling appears to be pretty reasonable, being roughly linear in $m$. And while being quadratic in $n$ sounds bad, one expects (or hopes!) that $n \\ll \\sqrt{m}$ in practical regression problems."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "locked": true,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**Fin!** If you've gotten this far without errors, your notebook is ready to submit."
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Create Assignment",
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}